# êµë³´ë¬¸ê³ ì—ì„œ ê°€ì¥ ì‘ì€ ë¶„ë¥˜ ë‹¨ìœ„ë¡œ í¬ë¡¤ë§ ëŒë¦¼
# ë¶„ë¥˜ì— í•´ë‹¹í•˜ëŠ” field codeë¥¼ field_list.txtì—ì„œ ìˆ˜ì • í›„ íŒŒì¼ ì‹¤í–‰ì‹œí‚¬ ê²ƒ

'''
Playwright ê¸°ë°˜ êµë³´ë¬¸ê³  ì „ì²´ í˜ì´ì§€ ì´ë¯¸ì§€ + ì—‘ì…€ í¬ë¡¤ëŸ¬

ì‹¤í–‰ ì „ ì„¤ì¹˜:
pip install playwright requests beautifulsoup4
playwright install
'''

import asyncio
import os
import requests
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright

async def crawl_category(field_code, browser, download_dir):
    print(f"\n===== ğŸ“š ë¶„ë¥˜ ì½”ë“œ {field_code} í¬ë¡¤ë§ ì‹œì‘ =====")
    base_url = f"https://product.kyobobook.co.kr/category/KOR/{field_code}#?page={{page}}&type=all&per=50&sort=sel"

    # í´ë” ìƒì„±
    image_dir = f"images/{field_code}"
    os.makedirs(image_dir, exist_ok=True)

    # Playwright ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ì„¤ì • (ì—‘ì…€ ë‹¤ìš´ë¡œë“œ í—ˆìš©)
    context = await browser.new_context(accept_downloads=True)
    page = await context.new_page()

    # âœ… 1. ì²« í˜ì´ì§€ ì ‘ì†
    await page.goto(base_url.format(page=1))
    await page.wait_for_timeout(3000)

    # âœ… 2. ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
    try:
        async with page.expect_download() as download_info:
            await page.click('button:has-text("Excelë‹¤ìš´ë¡œë“œ")')
        download = await download_info.value
        save_path = os.path.join(download_dir, f"meta_{field_code}.xlsx")
        await download.save_as(save_path)
        print(f"ğŸ“¥ ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {save_path}")
    except Exception as e:
        print(f"[â—ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜] {e}")

    # âœ… 3. ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
    html = await page.content()
    soup = BeautifulSoup(html, "html.parser")
    last_btn = soup.select_one('.btn_page_num[data-role="last"]')
    if last_btn and last_btn.text.strip().isdigit():
        last_page = int(last_btn.text.strip())
    else:
        last_page = 1
    print(f"ğŸ” ì „ì²´ í˜ì´ì§€ ìˆ˜: {last_page}")

    # âœ… 4. ì „ì²´ í˜ì´ì§€ ìˆœíšŒí•˜ë©° ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    for page_num in range(1, last_page + 1):
        print(f"â–¶ í˜ì´ì§€ {page_num} ì´ë¯¸ì§€ í¬ë¡¤ë§ ì¤‘...")
        await page.goto(base_url.format(page=page_num))
        await page.wait_for_timeout(5000)

        html = await page.content()
        soup = BeautifulSoup(html, "html.parser")
        items = soup.select(".prod_item")

        for item in items:
            try:
                link = item.find("a", class_="prod_link")
                if not link:
                    continue
                book_id = link["href"].split("/")[-1].strip()

                img_tag = item.select_one(".img_box img")
                if img_tag:
                    img_url = img_tag.get("src") or img_tag.get("data-src")
                    if img_url:
                        img_data = requests.get(img_url).content
                        with open(f"{image_dir}/{book_id}.jpg", "wb") as f:
                            f.write(img_data)
            except Exception as e:
                print(f"[â— ì´ë¯¸ì§€ ì €ì¥ ì˜¤ë¥˜] {e}")
                continue

    await context.close()

async def main():
    # í´ë” ìƒì„±
    os.makedirs("images", exist_ok=True)
    os.makedirs("excel_data", exist_ok=True)

    # ë¶„ë¥˜ì½”ë“œ ëª©ë¡ ë¡œë”©
    with open("field_list.txt", "r", encoding="utf-8") as f:
        field_codes = [line.strip() for line in f if line.strip()]

    # ë¸Œë¼ìš°ì € ì‹¤í–‰
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        for field_code in field_codes:
            await crawl_category(field_code, browser, "excel_data")
        await browser.close()

    print("\nğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ!")

# ì‹¤í–‰
asyncio.run(main())

