import asyncio
import os
import requests
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright

async def crawl_category(field_code, browser):
    print(f"\n===== ğŸ“š ë¶„ë¥˜ ì½”ë“œ {field_code} í¬ë¡¤ë§ ì‹œì‘ =====")
    base_url = f"https://product.kyobobook.co.kr/category/KOR/{field_code}"

    # âœ… í´ë” ìƒì„± (ë¶„ë¥˜ì½”ë“œë³„)
    image_dir = os.path.join("bestseller_images", field_code)
    excel_dir = "bestseller_meta"
    os.makedirs(image_dir, exist_ok=True)
    os.makedirs(excel_dir, exist_ok=True)

    context = await browser.new_context(accept_downloads=True)
    page = await context.new_page()

    try:
        # âœ… 1. ë² ìŠ¤íŠ¸ì…€ëŸ¬ íƒ­ ì ‘ì†
        await page.goto(f"{base_url}#homeTabBest?page=1&type=best&per=50")
        await page.wait_for_timeout(4000)

        # âœ… 2. ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
        try:
            container = page.locator('#homeTabBest')
            button = container.locator('button.excel_down')
            await button.scroll_into_view_if_needed()
            await page.wait_for_timeout(1000)
            async with page.expect_download() as download_info:
                await button.click()
            download = await download_info.value
            save_path = os.path.join(excel_dir, f"best_meta_{field_code}.xlsx")
            await download.save_as(save_path)
            print(f"ğŸ“¥ ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {save_path}")
        except Exception as e:
            print(f"[â—ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜] {e}")

        # âœ… 3. ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
        html = await page.content()
        soup = BeautifulSoup(html, "html.parser")
        last_btn = soup.select_one('.btn_page_num[data-role="last"]')
        last_page = int(last_btn.text.strip()) if last_btn and last_btn.text.strip().isdigit() else 1
        print(f"ğŸ” ì „ì²´ í˜ì´ì§€ ìˆ˜: {last_page}")

        # âœ… 4. ì´ë¯¸ì§€ í¬ë¡¤ë§
        for page_num in range(1, last_page + 1):
            print(f"â–¶ í˜ì´ì§€ {page_num} ì´ë¯¸ì§€ í¬ë¡¤ë§ ì¤‘...")
            await page.goto(f"{base_url}#homeTabBest?page={page_num}&type=best&per=50")
            await page.wait_for_timeout(4000)

            html = await page.content()
            soup = BeautifulSoup(html, "html.parser")
            items = soup.select(".prod_item")

            for item in items:
                try:
                    link = item.find("a", class_="prod_link")
                    if not link:
                        continue
                    book_id = link["href"].split("/")[-1].strip()
                    img_tag = item.select_one(".img_box img")
                    img_url = img_tag.get("src") or img_tag.get("data-src") if img_tag else None
                    if img_url:
                        img_data = requests.get(img_url).content
                        with open(os.path.join(image_dir, f"{book_id}.jpg"), "wb") as f:
                            f.write(img_data)
                except Exception as e:
                    print(f"[â— ì´ë¯¸ì§€ ì €ì¥ ì˜¤ë¥˜] {e}")
                    continue

    finally:
        await context.close()

async def main():
    with open("bestseller_field_list.txt", "r", encoding="utf-8") as f:
        field_codes = [line.strip() for line in f if line.strip()]

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        for field_code in field_codes:
            await crawl_category(field_code, browser)
        await browser.close()

    print("\nğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(main())
